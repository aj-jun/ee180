Write a README file that contains the following:
- Your name, your partner's name, your board's host name (e.g. ee180-10z.stanford.edu)
Jun, Ashley
ajjun@stanford.edu
Thai, Vincent
thaiv47@stanford.edu

Board hostname: ee180-3z.stanford.edu

- Each partner's contribution to the assignment
Ashley: Did the original single thread reconstruction and multi thread execution with its row partitioning across threads, barrier
syncronization, deterministic execution. Helped debug correctness issues, validate perfomance
counters

Vincent: Clarified the NEON vectorization in grayscale and sobelcalc for single thread,
reading in the BGR pixels and overflow handling with widen/calc/narrow, removing 
recalculations of row indices, and checking final performance results

- A brief description of optimizations you tried, the outcome, and some explanation (why it worked / why it did not)
Optimizations Tried, Outcomes, and Explanation

All optimizations were first applied to the single-threaded implementation, since the 
multi-threaded version builds directly on the optimized single-thread kernel. We began 
by modifying compiler flags based on ARM documentation and the lab handout 
(-O3, -mfpu=neon, -march=armv7-a, -mtune=cortex-a9). While these flags enabled 
vectorization and improved instruction scheduling, they did not significantly
increase FPS on their own because the baseline sobel_calc.cpp did not expose 
enough data-level parallelism for the compiler to exploit.

We then restructured the grayscale conversion. An early attempt collapsed the original 
nested loops into a single loop that directly segmented BGR values, which increased FPS
only marginally (approximately 3 to 5 FPS). This indicated that grayscale was not the 
primary bottleneck. We next introduced NEON intrinsics, initially loading 4 pixels at a
time using vld3_u8. This improved instruction throughput but still fell short of the 
target performance. Expanding the vector width to 8 pixels improved throughput further, 
but floating-point arithmetic limited gains. To address this, we converted grayscale 
computation to fixed-point integer arithmetic using scaled coefficients 
(7/64, 38/64, 19/64) with a right shift. This required widening pixel values to 16 bits 
to avoid overflow and narrowing back to 8 bits. This change significantly reduced 
instruction count while preserving correctness.

The largest performance improvement came from optimizing sobelCalc. We vectorized the 
Sobel filter by processing eight pixels at a time and computing both X and Y gradients 
using signed 16-bit arithmetic. Early versions attempted to combine gradients before 
saturation, which produced visually incorrect, grainy output and revealed the importance 
of preserving the reference implementationâ€™s intermediate clamping semantics. The final 
implementation computes gx and gy separately, takes absolute values, sums them, and 
applies saturating narrowing to match the reference behavior. A scalar fallback path 
handles remaining pixels that do not align with the vector width. With these changes, 
single-thread performance reached approximately 78 FPS. The same optimized kernel was 
then parallelized across two cores using row partitioning and pthread barriers, yielding 
near-linear scaling.

Final Performance Results

Single Thread:

Percent of time per function
Capture, 40.4224%
Grayscale, 19.8305%
Sobel, 19.2391%
Display, 20.5079%

Summary
Frames per second, 78.1091
Cycles per frame, 1.15256e+07
Energy per frames (mJ), 17.9236
Total frames, 50

Hardware Stats (Cap + Gray + Sobel + Display)
Instructions per cycle, 0.732002
L1 misses per frame, 126819
L1 misses per instruction, 0.0151693
Instruction count per frame, 8.36026e+06

Multithread:

Percent of time per function
Capture, 50.2018%
Grayscale, 12.6453%
Sobel, 11.9744%
Display, 25.1785%

Summary
Frames per second, 96.9277
Cycles per frame, 9.40407e+06
Energy per frames (mJ), 28.8875
Total frames, 50

Hardware Stats (Cap + Gray + Sobel + Display)
Instructions per cycle, 0.76885
L1 misses per frame, 95895.5
L1 misses per instruction, 0.0134544
Instruction count per frame, 7.12746e+06


- If you used AI tools, explain how you use it to help with the assignment
Ashley: 
Overall, I was confused by what the three different methods of optimization meant 
based on what was provided in the review slide deck. I copied and pasted the three 
methods of optimization and had chatgpt explain what each meant in a different way 
and how they were different from one another. 

I used AI to help provide defintions when explaining the different compiler flags. 
This was particularly helpful when understadnignt aht we can specify which architecture 
and processor to optimize for in the flags. 

Vincent:
Chat helped me understand the syntax/meaning of the NEON commands as I used them,
and also conceptually more about what they were doing. I also asked chat to help summarize/
inventory what kind of optimizations we did, though I am writing this myself. I also had
chat explain the makefile and conceptually what a sobel filter was